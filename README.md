# Micrograd: Autograd Engine from Scratch
This repository contains my implementation of Andrej Karpathy's Micrograd tutorial. Micrograd is a minimalistic engine for backpropagation and automatic differentiation, which is used to implement and train neural networks from scratch.

## Overview
Micrograd is designed to provide an intuitive understanding of how backpropagation and automatic differentiation work behind the scenes in neural networks. This project breaks down complex concepts into simpler building blocks, all written in pure Python without any external dependencies like TensorFlow or PyTorch.

### Key Features
Backpropagation: Implementation of the backward pass, allowing for automatic differentiation of scalar-valued functions.
Neural Networks: A basic neural network model built using manually defined layers and activation functions.
Minimalist Code: The entire project is implemented in under 100 lines of Python code, focusing on simplicity and clarity.

## Learnings
- This project helped solidify my understanding of:
- How automatic differentiation enables training neural networks.
- The inner workings of backpropagation algorithms.
- How neural networks can be constructed manually with simple building blocks.

# Acknowledgements
A big thank you to Andrej Karpathy for the amazing Micrograd tutorial and his contributions to making deep learning more accessible!
